rm(dec)
gc
gc()
gc(rm(list))
gc(rm())
gc(reset = T)
view(DF)
View(DF)
DF <- read.csv("C:\\Users\\abhij\\Desktop\\r_bootcamp_data\\collegeCompletion.csv",header=T)
DF <- read.csv("C:\\Users\\abhij\\Desktop\\r_bootcamp_data\\collegeCompletion.csv",header=T, na.rm=T)
apply(na.omit(DF),5,max)
DF <- read.csv("C:\\Users\\abhij\\Desktop\\r_bootcamp_data\\collegeCompletion.csv",header=T)
apply(na.omit(DF),5,max)
apply(na.omit(DF)
View(DF)
View(DF)
DF <- apply(na.omit(DF),5,max)
DF <- apply(na.omit(DF))
DF <- apply(DF,5,na.omit)
na.rm(DF)
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
View(x)
dimnames(x)[[1]] <- letters[1:8]
View(x)
apply(x, 2, mean, trim = .2)
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
x <- data.fram(cbind(x1 = 3, x2 = c(4:1, 2:5)))
x <- data.frame(cbind(x1 = 3, x2 = c(4:1, 2:5)))
View(x)
dimnames(x)[[1]] <- letters[1:8]
View(x)
apply(x, 2, mean)
col.sums <- apply(x, 2, sum)
row.sums <- apply(x, 1, sum)
DF[is.na(d)] <- 0
sum(DF[:5])
DF[:5]
DF[1:5]
sum(x[:2])
x[:2]
DF <- read.csv("C:\\Users\\abhij\\Desktop\\r_bootcamp_data\\collegeCompletion.csv",header=T)
DF[is.na(d)] <- 0
DF[is.na(DF)] <- 0
View(DF)
View(DF)
sum(DF[:10])
colSums(DF,na.rm=T,dims=10)
colSums(DF,na.rm=T,dims=1)
colSums(DF[:10],na.rm=T,dims=1)
colSums(DF{:10},na.rm=T,dims=1)
DF[:10]
DF[ :10]
DF[10]
DF[1:10]
DF[10]
DF[:10]
DF[:1]
DF[10]
DF[,10]
DF[20,]
sum(DF[,10], na.rm=T)
install.packages("tidyverse")
gc(rm())
gc(rm(env))
rm(env)
rm(ls())
rm(list=ls())
?assignOps
library(tidyverse)
search()
library(readxl)
setwd("C:\Users\abhij\Desktop\UVa Coursework\Bootcamp\R\r_bootcamp_data")
47*2.25
50*2.25
lapply(list(1,0.1,0.001), function(x) (x*sin(1/x)))
lapply(list(1,0.1,0.001, 0.001), function(x) (x*sin(1/x)))
lapply(list(1,0.1,0.001, 0.001,0.0001), function(x) (x*sin(1/x)))
lapply(list(1,0.1,0.001, 0.001,0.0001,0.00001), function(x) (x*sin(1/x)))
sapply(list(1,0.1,0.001, 0.001,0.0001,0.00001), function(x) (x*sin(1/x)))
mapply(list(1,0.1,0.001, 0.001,0.0001,0.00001), function(x) (x*sin(1/x)))
apply(list(1,0.1,0.001, 0.001,0.0001,0.00001), function(x) (x*sin(1/x)))
apply(list(1,0.1,0.001, 0.001,0.0001,0.00001), 1, function(x) (x*sin(1/x)))
sapply(list(1,0.1,0.001, 0.001,0.0001,0.00001), function(x) (x*sin(1/x)))
library(tidyverse)
library(anytime)
library(sqldf)
library(ggplot2)
library(dplyr)
library(reshape)
#Read tsv file, choose class as character to avoid loss of information in UPC, order_time and order_id variables
data <- read.delim('4729-2038.tsv', colClasses = "character")
#Unix timestamp comes with millisec information.
#As they are all 0s, divide by 1000 and run the anytime package to give a usable format
data$order_time <- anytime(as.numeric(data$order_time)/1000)
#Strip order_time to give individual time points like, day, date, hour
data$DATE1=strptime(data$order_time, "%Y-%m-%d %H:%M:%S",tz="")
data$Day=format(data$DATE1, "%A")
data$DATE<-strftime(data$DATE,"%d/%m/%Y")
data$Hour=format(data$DATE1, "%H")
data$DATE1<-NULL
setwd("C:/Users/abhij/Desktop/Walmart Challenege")
data <- read.delim('4729-2038.tsv', colClasses = "character")
data$order_time <- anytime(as.numeric(data$order_time)/1000)
data$DATE1=strptime(data$order_time, "%Y-%m-%d %H:%M:%S",tz="")
data$Day=format(data$DATE1, "%A")
data$DATE<-strftime(data$DATE,"%d/%m/%Y")
data$Hour=format(data$DATE1, "%H")
data$DATE1<-NULL
cols = c("store_number", "department", "register","amount", "Hour");
data[,cols] = apply(data[,cols], 2, function(x) as.numeric(x))
bad_transactions <- data[data$amount<=0,]
summary(bad_transactions)
data <- data[!data$amount<=0, ]
dept_35 <- data[data$department == 35,]
sum(dept_35$amount)
data <- data[!data$department == c(86,35), ]
walmart <- data[data$store_number == 2222,]
#Since we have 2 days worth of data, an hour by hour trend analysis would make the most sense.
#the same kind of analysis could easily be applied across
#any time period, type of stores, geographical location given data.
hour_bill_trend <- sqldf("select Day, Hour, count(distinct order_id) as unique_bills,
sum(amount) as sales, count(upc) as products from walmart group by 1,2")
#The above summary contains Day-wise, hour-wise break up of the distinct bills cut,
#Total sales in that period and number of products sold
#Atomize sales/hour to give a clear picture of the most productive hour across each day
hour_bill_trend$bill_value <- hour_bill_trend$sales/hour_bill_trend$unique_bills
#Plot produced is a hour wise line chart each day in the data set
ggplot(hour_bill_trend, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
# ggplot(hour_bill_trend, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#
# ggplot(hour_bill_trend, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#I have commented out the other plots for products and bills as they don't seem to ass anything new
#to what information we can gain from sales plot itself
#Reading into it, it's clear the busiest time at the store is
#between 4-7pm on a Saturday easily breaching the $20k/hour mark. Probably preparing for a party or dinner
#Further, since the total sales and bills cut on a weekend is roughly equal to the total sales on the weekdays
#it's safe to assume this is probably the busiest day of the week barring,
#Black Fridays or special discount weekends when the traffic could be higher
#We can see delayed onset on Friday, 7-9 would be it's peak
#owing to the some last min shopping on their way back home for the weekend
#Sunday afternoon matches it's Saturday counterpart but only at noon otherwise it seems to fall off sharply.
#I'd call this the death due to the Monday Morning Dread
#Lets see what sold on Saturday 4-7pm
peak <- data[data$Day == "Saturday" & data$Hour >= 16 & data$Hour < 19,]
#Those 3 hours make up a huge 20.68% of all sales in the given 48 hour period
sum(peak$amount)/sum(data$amount)
#Summarize department trend based on sales.
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
#Check top 3 departments with combination of bills and sales
check_95 <- unique(peak[peak$department == 95,])
check_2 <- unique(peak[peak$department == 2,])
check_40 <- unique(peak[peak$department == 40,])
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
#Department 2 or Personal Care rakes in the highest sales and has 418 bills which is 2nd best after
#Department 95 or Party snack and mixers. Probably preparing for evening/night ahead.
#Department 40 was Water, energy drinks, vitamins. Looks like people want to be well hydrated through the party
sunday <- data[data$Day == "Sunday",]
friday <- data[data$Day == "Sunday",]
department_trend_sun <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from sunday group by 1")
department_trend_fri <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from friday group by 1")
View(department_trend_sun)
check_95 <- peak[peak$department == 95,]
check_40 <- peak[peak$department == 40,]
check_2 <- peak[peak$department == 2,]
View(check_95)
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
View(sat_peak_products)
null2 <- data[data$description == "NULL",]
View(null2)
data <- read.delim('4729-2038.tsv', colClasses = "character")
data$order_time <- anytime(as.numeric(data$order_time)/1000)
data$DATE1=strptime(data$order_time, "%Y-%m-%d %H:%M:%S",tz="")
data$Day=format(data$DATE1, "%A")
data$DATE<-strftime(data$DATE,"%d/%m/%Y")
data$Hour=format(data$DATE1, "%H")
data$DATE1<-NULL
#convert required variables back to numeric since they were all imported as character
#to further process them
cols = c("store_number", "department", "register","amount", "Hour");
data[,cols] = apply(data[,cols], 2, function(x) as.numeric(x))
#Check summary of each variable to see if the basic statistics look right
summary(data)
#Turns out 938 or ~1% of the transactions were 0 or negative
#These transactions were most probably discounts/returns/free with another UPC
#Removing them gives a more accurate picture of the transactions
bad_transactions <- data[data$amount<=0,]
summary(bad_transactions)
data <- data[!data$amount<=0, ]
null2 <- data[data$description == "NULL",]
View(null2)
View(null2)
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
return (data$description[i] = data$name[i])
}
for (i in 1:nrow(data)){
if (data$description[i] == "NULL"){
return (data$description[i] = data$name[i])
}
}
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
{(data$description[i] = data$name[i])}
}
t -data[data$description == "NULL",]
t <- data[data$description == "NULL",]
#Call libraries used in the code
library(tidyverse)
library(anytime)
library(sqldf)
library(ggplot2)
library(dplyr)
library(reshape)
#Read tsv file, choose class as character to avoid loss of information in UPC, order_time and order_id variables
data <- read.delim('4729-2038.tsv', colClasses = "character")
#Unix timestamp comes with millisec information.
#As they are all 0s, divide by 1000 and run the anytime package to give a usable format
data$order_time <- anytime(as.numeric(data$order_time)/1000)
#Strip order_time to give individual time points like, day, date, hour
data$DATE1=strptime(data$order_time, "%Y-%m-%d %H:%M:%S",tz="")
data$Day=format(data$DATE1, "%A")
data$DATE<-strftime(data$DATE,"%d/%m/%Y")
data$Hour=format(data$DATE1, "%H")
data$DATE1<-NULL
#convert required variables back to numeric since they were all imported as character
#to further process them
cols = c("store_number", "department", "register","amount", "Hour");
data[,cols] = apply(data[,cols], 2, function(x) as.numeric(x))
#Check summary of each variable to see if the basic statistics look right
summary(data)
#Turns out 938 or ~1% of the transactions were 0 or negative
#These transactions were most probably discounts/returns/free with another UPC
#Removing them gives a more accurate picture of the transactions
bad_transactions <- data[data$amount<=0,]
summary(bad_transactions)
#Remove all bad transactions from the data set
data <- data[!data$amount<=0, ]
#Department-wise inspection producecd a set of 231 transactions which were unusually high
#A deeper look confirmed that they were money transfers and not actual upc transactions for goods sold.
#there were 1430 transactions demarcated as department 35 or 38 which had a NULL description
#I replaced those descriptions with the name to improve readability later on
dept_86 <- data[data$department == 86,]
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
{(data$description[i] = data$name[i])}
}
#I have removed them from this data set to avoid any skew in data because of these small but significant transactions
data <- data[!data$department == c(86,35), ]
#Analysis for store_number 2222 designated as walmart. Although it could easily be Sam's club
#2222 had ~75% of the transactions thus it made sense to investigate this first
##########################
#Subset data for store_number 2222
walmart <- data[data$store_number == 2222,]
#Since we have 2 days worth of data, an hour by hour trend analysis would make the most sense.
#the same kind of analysis could easily be applied across
#any time period, type of stores, geographical location given data.
hour_bill_trend <- sqldf("select Day, Hour, count(distinct order_id) as unique_bills,
sum(amount) as sales, count(upc) as products from walmart group by 1,2")
#The above summary contains Day-wise, hour-wise break up of the distinct bills cut,
#Total sales in that period and number of products sold
#Atomize sales/hour to give a clear picture of the most productive hour across each day
hour_bill_trend$bill_value <- hour_bill_trend$sales/hour_bill_trend$unique_bills
#Plot produced is a hour wise line chart each day in the data set
ggplot(hour_bill_trend, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
# ggplot(hour_bill_trend, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#
# ggplot(hour_bill_trend, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#I have commented out the other plots for products and bills as they don't seem to ass anything new
#to what information we can gain from sales plot itself
#Reading into it, it's clear the busiest time at the store is
#between 4-7pm on a Saturday easily breaching the $20k/hour mark. Probably preparing for a party or dinner
#Further, since the total sales and bills cut on a weekend is roughly equal to the total sales on the weekdays
#it's safe to assume this is probably the busiest day of the week barring,
#Black Fridays or special discount weekends when the traffic could be higher
#We can see delayed onset on Friday, 7-9 would be it's peak
#owing to the some last min shopping on their way back home for the weekend
#Sunday afternoon matches it's Saturday counterpart but only at noon otherwise it seems to fall off sharply.
#I'd call this the death due to the Monday Morning Dread
#Lets see what sold on Saturday 4-7pm
peak <- data[data$Day == "Saturday" & data$Hour >= 16 & data$Hour < 19,]
#Those 3 hours make up a huge 20.68% of all sales in the given 48 hour period
sum(peak$amount)/sum(data$amount)
#Summarize department trend based on sales.
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
#Check top 3 departments with combination of bills and sales
check_95 <- peak[peak$department == 95,]
check_2 <- peak[peak$department == 2,]
check_40 <- peak[peak$department == 40,]
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
#Department 2 or Personal Care rakes in the highest sales and has 418 bills which is 2nd best after
#Department 95 or Party snack and mixers. Probably preparing for evening/night ahead.
#Department 40 was Water, energy drinks, vitamins. Looks like people want to be well hydrated through the party
sunday <- data[data$Day == "Sunday",]
friday <- data[data$Day == "Sunday",]
department_trend_sun <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from sunday group by 1")
department_trend_fri <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from friday group by 1")
View(department_trend_fri)
View(sat_peak_products)
peak <- data[walmart$Day == "Saturday" & walmart$Hour >= 16 & walmart$Hour < 19,]
#Those 3 hours make up a huge 20.68% of all sales in the given 48 hour period
sum(peak$amount)/sum(data$amount)
#Summarize department trend based on sales.
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
#Check top 3 departments with combination of bills and sales
check_95 <- peak[peak$department == 95,]
check_2 <- peak[peak$department == 2,]
check_40 <- peak[peak$department == 40,]
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
#Department 2 or Personal Care rakes in the highest sales and has 418 bills which is 2nd best after
#Department 95 or Party snack and mixers. Probably preparing for evening/night ahead.
#Department 40 was Water, energy drinks, vitamins. Looks like people want to be well hydrated through the party
sunday <- data[data$Day == "Sunday",]
friday <- data[data$Day == "Sunday",]
department_trend_sun <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from sunday group by 1")
department_trend_fri <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from friday group by 1")
sum(peak$amount)/sum(data$amount)
View(sat_peak_products)
t <- data[data$description == grepl("ria", data$description, ignore.case = T)]
grepl("ria", data$description, ignore.case = T)
t <- data[data$description = grepl("ria", data$description, ignore.case = T)]
t <- data[data$description == grepl("ria", data$description, ignore.case = T)]
grep("ria", data$description, ignore.case = T)
t <- data[data$description[grep("ria", data$description, ignore.case = T)],]
View(t)
t <- data[data$description[c(grep("ria", data$description, ignore.case = T)]),]
t <- data[data$description[grep("ria", data$description, ignore.case = T)],]
t <- filter(data, !grepl("ria",description, ignore.case = T))
t <- filter(data, grepl("ria",description, ignore.case = T))
View(t)
t <- filter(data, grepl("ria send",description, ignore.case = T))
View(t)
data <- data[!data$department == 86 ]
t <- filter(data, grepl("ria send",description, ignore.case = T))
t <- filter(data, grepl("ria send",description, ignore.case = T))
data <- read.delim('4729-2038.tsv', colClasses = "character")
data$order_time <- anytime(as.numeric(data$order_time)/1000)
data$DATE1=strptime(data$order_time, "%Y-%m-%d %H:%M:%S",tz="")
data$Day=format(data$DATE1, "%A")
data$DATE<-strftime(data$DATE,"%d/%m/%Y")
data$Hour=format(data$DATE1, "%H")
data$DATE1<-NULL
#convert required variables back to numeric since they were all imported as character
#to further process them
cols = c("store_number", "department", "register","amount", "Hour");
data[,cols] = apply(data[,cols], 2, function(x) as.numeric(x))
bad_transactions <- data[data$amount<=0,]
data <- data[!data$amount<=0, ]
dept_86 <- data[data$department == 86,]
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
{(data$description[i] = data$name[i])}
}
data <- data[!data$department == 86 ]
data$department == 86
t <- filter(data, grepl("ria send",description, ignore.case = T))
View(t)
dept_86 <- data[data$department == 86,]
rm(dept_86)
dept_86 <- data[data$department == 86,]
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
{(data$description[i] = data$name[i])}
}
data <- data[!data$department == 86 ]
data <- data[!data$department == 86, ]
t <- filter(data, grepl("ria send",description, ignore.case = T))
walmart <- data[data$store_number == 2222,]
#Since we have 2 days worth of data, an hour by hour trend analysis would make the most sense.
#the same kind of analysis could easily be applied across
#any time period, type of stores, geographical location given data.
hour_bill_trend <- sqldf("select Day, Hour, count(distinct order_id) as unique_bills,
sum(amount) as sales, count(upc) as products from walmart group by 1,2")
#The above summary contains Day-wise, hour-wise break up of the distinct bills cut,
#Total sales in that period and number of products sold
#Atomize sales/hour to give a clear picture of the most productive hour across each day
hour_bill_trend$bill_value <- hour_bill_trend$sales/hour_bill_trend$unique_bills
#Plot produced is a hour wise line chart each day in the data set
ggplot(hour_bill_trend, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
# ggplot(hour_bill_trend, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#
# ggplot(hour_bill_trend, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#I have commented out the other plots for products and bills as they don't seem to ass anything new
#to what information we can gain from sales plot itself
#Reading into it, it's clear the busiest time at the store is
#between 4-7pm on a Saturday easily breaching the $20k/hour mark. Probably preparing for a party or dinner
#Further, since the total sales and bills cut on a weekend is roughly equal to the total sales on the weekdays
#it's safe to assume this is probably the busiest day of the week barring,
#Black Fridays or special discount weekends when the traffic could be higher
#We can see delayed onset on Friday, 7-9 would be it's peak
#owing to the some last min shopping on their way back home for the weekend
#Sunday afternoon matches it's Saturday counterpart but only at noon otherwise it seems to fall off sharply.
#I'd call this the death due to the Monday Morning Dread
#Lets see what sold on Saturday 4-7pm
peak <- data[walmart$Day == "Saturday" & walmart$Hour >= 16 & walmart$Hour < 19,]
#Those 3 hours make up a huge 15.67% of all sales in the given 48 hour period
sum(peak$amount)/sum(data$amount)
#Summarize department trend based on sales.
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
#Check top 3 departments with combination of bills and sales
check_95 <- peak[peak$department == 95,]
check_2 <- peak[peak$department == 2,]
check_40 <- peak[peak$department == 40,]
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
#Department 2 or Personal Care rakes in the highest sales and has 418 bills which is 2nd best after
#Department 95 or Party snack and mixers. Probably preparing for evening/night ahead.
#Department 40 was Water, energy drinks, vitamins. Looks like people want to be well hydrated through the party
sunday <- data[data$Day == "Sunday",]
friday <- data[data$Day == "Sunday",]
department_trend_sun <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from sunday group by 1")
department_trend_fri <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from friday group by 1")
View(sat_peak_products)
View(department_trend_sat_peak)
check_92 <- peak[peak$department == 92,]
View(check_92)
View(sat_peak_products)
View(check_92)
View(check_95)
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from check_92 group by 1")
View(sat_peak_products)
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
View(department_trend_sat_peak)
View(check_92)
dept_92 <- sqldf("select description, count(description) as count,
sum(amount) as sales from check_92 group by 1")
View(dept_92)
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
View(sat_peak_products)
sat_peak_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from peak group by 1,2")
View(sat_peak_products)
dept_99 <- peak[peak$department == 99,]
View(check_40)
View(dept_99)
sat_peak_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from peak group by 1,2")
View(sat_peak_products)
View(department_trend_sun)
View(department_trend_fri)
sun_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from sunday group by 1,2")
View(sun_products)
View(sat_peak_products)
fri_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from sunday group by 1,2")
View(fri_products)
registers <-group_by(walmart, walmart$register) %>% summarize(amount = n())
View(registers)
registers_peak <-group_by(peak, peak$register) %>% summarize(amount = n())
View(registers_peak)
registers <- walmart %>%
select(Day, Hour, register) %>%
group_by(register) %>%
summarise(sales = sum(amount), bills = unique(order_id))
View(walmart)
registers <- walmart %>%
select(Day, Hour, register) %>%
group_by(register) %>%
summarise(sales = sum(walmart$amount), bills = unique(walmart$order_id))
registers <- walmart %>%
select(Day, Hour, register) %>%
group_by(register) %>%
summarise(sales = sum(walmart$amount), bills = count(walmart$order_id))
registers_peak <-group_by(peak, peak$register) %>% summarize(amount = n())
View(registers_peak)
View(registers)
View(registers_peak)
registers_peak <-group_by(peak, (peak$register) as register) %>% summarize(amount = n())
registers_peak <-group_by(peak, (peak$register) = register) %>% summarize(amount = n())
registers_peak <-group_by(peak, (peak$register) == register) %>% summarize(amount = n())
View(registers_peak)
registers_peak <-group_by(peak, register) %>% summarize(amount = n())
View(registers_peak)
summary(lm(register~amount, data =registers_peak))
registers <-group_by(walmart, register) %>% summarize(amount = n())
summary(lm(register~amount, data =registers))
ggplot(hour_bill_trend, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
ggplot(hour_bill_trend, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
ggplot(hour_bill_trend, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
