data$order_time <- anytime(as.numeric(data$order_time)/1000)
data$DATE1=strptime(data$order_time, "%Y-%m-%d %H:%M:%S",tz="")
data$Day=format(data$DATE1, "%A")
data$DATE<-strftime(data$DATE,"%d/%m/%Y")
data$Hour=format(data$DATE1, "%H")
data$DATE1<-NULL
#convert required variables back to numeric since they were all imported as character
#to further process them
cols = c("store_number", "department", "register","amount", "Hour");
data[,cols] = apply(data[,cols], 2, function(x) as.numeric(x))
bad_transactions <- data[data$amount<=0,]
data <- data[!data$amount<=0, ]
dept_86 <- data[data$department == 86,]
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
{(data$description[i] = data$name[i])}
}
data <- data[!data$department == 86 ]
data$department == 86
t <- filter(data, grepl("ria send",description, ignore.case = T))
View(t)
dept_86 <- data[data$department == 86,]
rm(dept_86)
dept_86 <- data[data$department == 86,]
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
{(data$description[i] = data$name[i])}
}
data <- data[!data$department == 86 ]
data <- data[!data$department == 86, ]
t <- filter(data, grepl("ria send",description, ignore.case = T))
walmart <- data[data$store_number == 2222,]
#Since we have 2 days worth of data, an hour by hour trend analysis would make the most sense.
#the same kind of analysis could easily be applied across
#any time period, type of stores, geographical location given data.
hour_bill_trend <- sqldf("select Day, Hour, count(distinct order_id) as unique_bills,
sum(amount) as sales, count(upc) as products from walmart group by 1,2")
#The above summary contains Day-wise, hour-wise break up of the distinct bills cut,
#Total sales in that period and number of products sold
#Atomize sales/hour to give a clear picture of the most productive hour across each day
hour_bill_trend$bill_value <- hour_bill_trend$sales/hour_bill_trend$unique_bills
#Plot produced is a hour wise line chart each day in the data set
ggplot(hour_bill_trend, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
# ggplot(hour_bill_trend, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#
# ggplot(hour_bill_trend, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
#   scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#I have commented out the other plots for products and bills as they don't seem to ass anything new
#to what information we can gain from sales plot itself
#Reading into it, it's clear the busiest time at the store is
#between 4-7pm on a Saturday easily breaching the $20k/hour mark. Probably preparing for a party or dinner
#Further, since the total sales and bills cut on a weekend is roughly equal to the total sales on the weekdays
#it's safe to assume this is probably the busiest day of the week barring,
#Black Fridays or special discount weekends when the traffic could be higher
#We can see delayed onset on Friday, 7-9 would be it's peak
#owing to the some last min shopping on their way back home for the weekend
#Sunday afternoon matches it's Saturday counterpart but only at noon otherwise it seems to fall off sharply.
#I'd call this the death due to the Monday Morning Dread
#Lets see what sold on Saturday 4-7pm
peak <- data[walmart$Day == "Saturday" & walmart$Hour >= 16 & walmart$Hour < 19,]
#Those 3 hours make up a huge 15.67% of all sales in the given 48 hour period
sum(peak$amount)/sum(data$amount)
#Summarize department trend based on sales.
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
#Check top 3 departments with combination of bills and sales
check_95 <- peak[peak$department == 95,]
check_2 <- peak[peak$department == 2,]
check_40 <- peak[peak$department == 40,]
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
#Department 2 or Personal Care rakes in the highest sales and has 418 bills which is 2nd best after
#Department 95 or Party snack and mixers. Probably preparing for evening/night ahead.
#Department 40 was Water, energy drinks, vitamins. Looks like people want to be well hydrated through the party
sunday <- data[data$Day == "Sunday",]
friday <- data[data$Day == "Sunday",]
department_trend_sun <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from sunday group by 1")
department_trend_fri <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from friday group by 1")
View(sat_peak_products)
View(department_trend_sat_peak)
check_92 <- peak[peak$department == 92,]
View(check_92)
View(sat_peak_products)
View(check_92)
View(check_95)
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from check_92 group by 1")
View(sat_peak_products)
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
View(department_trend_sat_peak)
View(check_92)
dept_92 <- sqldf("select description, count(description) as count,
sum(amount) as sales from check_92 group by 1")
View(dept_92)
sat_peak_products <- sqldf("select description, count(description) as count,
sum(amount) as sales from peak group by 1")
View(sat_peak_products)
sat_peak_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from peak group by 1,2")
View(sat_peak_products)
dept_99 <- peak[peak$department == 99,]
View(check_40)
View(dept_99)
sat_peak_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from peak group by 1,2")
View(sat_peak_products)
View(department_trend_sun)
View(department_trend_fri)
sun_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from sunday group by 1,2")
View(sun_products)
View(sat_peak_products)
fri_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from sunday group by 1,2")
View(fri_products)
registers <-group_by(walmart, walmart$register) %>% summarize(amount = n())
View(registers)
registers_peak <-group_by(peak, peak$register) %>% summarize(amount = n())
View(registers_peak)
registers <- walmart %>%
select(Day, Hour, register) %>%
group_by(register) %>%
summarise(sales = sum(amount), bills = unique(order_id))
View(walmart)
registers <- walmart %>%
select(Day, Hour, register) %>%
group_by(register) %>%
summarise(sales = sum(walmart$amount), bills = unique(walmart$order_id))
registers <- walmart %>%
select(Day, Hour, register) %>%
group_by(register) %>%
summarise(sales = sum(walmart$amount), bills = count(walmart$order_id))
registers_peak <-group_by(peak, peak$register) %>% summarize(amount = n())
View(registers_peak)
View(registers)
View(registers_peak)
registers_peak <-group_by(peak, (peak$register) as register) %>% summarize(amount = n())
registers_peak <-group_by(peak, (peak$register) = register) %>% summarize(amount = n())
registers_peak <-group_by(peak, (peak$register) == register) %>% summarize(amount = n())
View(registers_peak)
registers_peak <-group_by(peak, register) %>% summarize(amount = n())
View(registers_peak)
summary(lm(register~amount, data =registers_peak))
registers <-group_by(walmart, register) %>% summarize(amount = n())
summary(lm(register~amount, data =registers))
ggplot(hour_bill_trend, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
ggplot(hour_bill_trend, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
ggplot(hour_bill_trend, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#Call libraries used in the code
library(tidyverse)
library(anytime)
library(sqldf)
library(ggplot2)
library(dplyr)
library(reshape)
#Read tsv file, choose class as character to avoid loss of information in UPC, order_time and order_id variables
data <- read.delim('4729-2038.tsv', colClasses = "character")
#Unix timestamp comes with millisec information.
#As they are all 0s, divide by 1000 and run the anytime package to give a usable format
data$order_time <- anytime(as.numeric(data$order_time)/1000)
#Strip order_time to give individual time points like, day, date, hour
data$DATE1=strptime(data$order_time, "%Y-%m-%d %H:%M:%S",tz="")
data$Day=format(data$DATE1, "%A")
data$DATE<-strftime(data$DATE,"%d/%m/%Y")
data$Hour=format(data$DATE1, "%H")
data$DATE1<-NULL
#convert required variables back to numeric since they were all imported as character
#to further process them
cols = c("store_number", "department", "register","amount", "Hour");
data[,cols] = apply(data[,cols], 2, function(x) as.numeric(x))
#Check summary of each variable to see if the basic statistics look right
summary(data)
#Turns out 938 or ~1% of the transactions were 0 or negative
#These transactions were most probably discounts/returns/free with another UPC
#Removing them gives a more accurate picture of the transactions
bad_transactions <- data[data$amount<=0,]
summary(bad_transactions)
#Remove all bad transactions from the data set
data <- data[!data$amount<=0, ]
#Department-wise inspection producecd a set of 231 transactions which were unusually high
#A deeper look confirmed that they were money transfers and not actual upc transactions for goods sold.
#there were 1430 transactions demarcated as department 35 or 38 which had a NULL description
#I replaced those descriptions with the name to improve readability later on
dept_86 <- data[data$department == 86,]
for (i in 1:nrow(data)){
if (data$description[i] == "NULL")
{(data$description[i] = data$name[i])}
}
#I have removed them from this data set to avoid
#any skew in data because of these small but significant transactions
data <- data[!data$department == 86, ]
#t <- filter(data, grepl("ria send",description, ignore.case = T))
#Analysis for store_number 2222 designated as walmart. Although it could easily be Sam's club
#2222 had ~75% of the transactions thus it made sense to investigate this first
##########################
#Subset data for store_number 2222
walmart <- data[data$store_number == 2222,]
#Since we have 2 days worth of data, an hour by hour trend analysis would make the most sense.
#the same kind of analysis could easily be applied across
#any time period, type of stores, geographical location given data.
hour_bill_trend <- sqldf("select Day, Hour, count(distinct order_id) as unique_bills,
sum(amount) as sales, count(upc) as products from walmart group by 1,2")
#The above summary contains Day-wise, hour-wise break up of the distinct bills cut,
#Total sales in that period and number of products sold
#Atomize sales/hour to give a clear picture of the most productive hour across each day
hour_bill_trend$bill_value <- hour_bill_trend$sales/hour_bill_trend$unique_bills
#Plot produced is a hour wise line chart each day in the data set
ggplot(hour_bill_trend, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
ggplot(hour_bill_trend, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
ggplot(hour_bill_trend, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend$Hour), max(hour_bill_trend$Hour)))
#The other plots for products and bills don't seem to ass anything new
#to what information we can gain from sales plot itself
#Reading into it, it's clear the busiest time at the store is
#between 4-7pm on a Saturday easily breaching the $20k/hour mark. Probably preparing for a party or dinner
#Further, since the total sales and bills cut on a weekend is roughly equal to the total sales on the weekdays
#it's safe to assume this is probably the busiest day of the week barring,
#Black Fridays or special discount weekends when the traffic could be higher
#We can see delayed onset on Friday, 7-9 would be it's peak
#owing to the some last min shopping on their way back home for the weekend
#Sunday afternoon matches it's Saturday counterpart but only at noon otherwise it seems to fall off sharply.
#I'd call this the death due to the Monday Morning Dread
###################################
#Lets see what sold on Saturday 4-7pm
###################################
peak <- data[walmart$Day == "Saturday" & walmart$Hour >= 16 & walmart$Hour < 19,]
#Those 3 hours make up a huge 15.67% of all sales in the given 48 hour period
sum(peak$amount)/sum(data$amount)
#Summarize department trend based on sales.
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
#Check top 4 departments with combination of bills and sales
dept_95 <- peak[peak$department == 95,]
dept_2 <- peak[peak$department == 2,]
dept_40 <- peak[peak$department == 40,]
dept_92 <- peak[peak$department == 92,]
#Department 2 or Personal Care rakes in the highest sales and has 418 bills which is 2nd best after
#Department 95 or Party snack and mixers. Probably preparing for evening/night ahead.
#Department 40 was Water, energy drinks, vitamins. Looks like people want to be well hydrated through the party
#Department 92 or homefoods like Ramen or cereal made up the rest of the majority
sat_peak_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from peak group by 1,2")
#Drilling down the most bought items were from the pharmacy and Gasoline,
#Reproducing this for Friday and Sunday to see if there were any changes in buying patterns
#comapred to peak time on Saturday
sunday <- data[data$Day == "Sunday",]
friday <- data[data$Day == "Sunday",]
department_trend_sun <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from sunday group by 1")
sun_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from sunday group by 1,2")
department_trend_fri <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from friday group by 1")
fri_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from sunday group by 1,2")
#The top 4 departments and top products remained unchanged
###########################
#Investigating the register
###########################
#Summarize by register to check performance
registers <-group_by(walmart, register) %>% summarize(amount = n())
#As expected there is no linear relationship between the number, thus placement and sales
summary(lm(register~amount, data =registers))
###########################
#Market Basket
###########################
sku_bills<-sqldf("select description, count(distinct order_id) as bills from walmart group by 1")
#subcat bills overal
subcat_bills<-sqldf("select department, count(distinct order_id) as bills from walmart group by 1")
#total_bills
total_bills<-sqldf("select count(distinct order_id) as bills from data")
#marketbasket
prod_id<-sqldf("select order_id, descreption from walmart group by 1,2")
library(arules)
library(arulesViz)
names(prod_id)[names(prod_id) == 'description'] <- 'prod_id'
names(prod_id)[names(prod_id) == 'order_id'] <- 'trans_id'
write.csv(prod_id, "prod_id.csv",row.names=FALSE)
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.0001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame");
write.csv(final, "prod_rules.csv", row.names=FALSE, na="")
write.csv(subcat_bills,"subcat_bills.csv",row.names=F)
write.csv(sku_bills,"sku_bills.csv",row.names=F)
write.csv(total_bills,"total_bills.csv",row.names=F)
prod_id<-sqldf("select order_id, description from walmart group by 1,2")
library(arules)
library(arulesViz)
names(prod_id)[names(prod_id) == 'description'] <- 'prod_id'
names(prod_id)[names(prod_id) == 'order_id'] <- 'trans_id'
write.csv(prod_id, "prod_id.csv",row.names=FALSE)
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.0001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame");
write.csv(final, "prod_rules.csv", row.names=FALSE, na="")
write.csv(subcat_bills,"subcat_bills.csv",row.names=F)
write.csv(sku_bills,"sku_bills.csv",row.names=F)
write.csv(total_bills,"total_bills.csv",row.names=F)
install.packages("arules")
install.packages("arulesViz")
library(arules)
library(arulesViz)
prod_id<-sqldf("select order_id, description from walmart group by 1,2")
names(prod_id)[names(prod_id) == 'description'] <- 'prod_id'
names(prod_id)[names(prod_id) == 'order_id'] <- 'trans_id'
write.csv(prod_id, "prod_id.csv",row.names=FALSE)
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.0001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame");
write.csv(final, "prod_rules.csv", row.names=FALSE, na="")
View(final)
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.0001,conf = 0.005,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame");
View(dept_92)
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame");
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.0001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame");
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame");
View(final)
View(final)
WM<-sqldf("select order_id, description from walmart group by 1,2")
names(WM)[names(WM) == 'description'] <- 'prod_id'
names(WM)[names(WM) == 'order_id'] <- 'trans_id'
#write.csv(prod_id, "prod_id.csv",row.names=FALSE)
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame")
WM = read.transactions(WM, format = "single", sep = ",", cols = c(1,2))
prod_id<-sqldf("select order_id, description from walmart group by 1,2")
names(prod_id)[names(prod_id) == 'description'] <- 'prod_id'
names(prod_id)[names(prod_id) == 'order_id'] <- 'trans_id'
write.csv(prod_id, "prod_id.csv",row.names=FALSE)
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
rules<-apriori(data=WM, parameter=list(supp=0.001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
rules<-sort(rules, decreasing=TRUE,by="confidence")
inspect(rules[1:5])
final = as(rules, "data.frame")
View(friday)
termMatrix <- final.mat %*% t(final.mat)
termMatrix <- final %*% t(final)
sel <- plot(rules, measure=c("support", "lift"), shading="confidence", interactive=TRUE)
sel <- plot(rules, measure=c("support", "lift"), shading="confidence", interactive=TRUE)
library(igraph)
subrules2 <- head(sort(rules, by="lift"), 10)
plot(subrules2, method="graph")
subrules2 <- head(sort(rules, by="lift"), 100)
plot(subrules2, method="graph")
inspect(rules[1:5])
subrules2 <- head(sort(rules, by="lift"), 50)
plot(subrules2, method="graph")
subrules2 <- head(sort(rules, by="lift"), 20)
plot(subrules2, method="graph")
prod_id<-sqldf("select order_id as trans_id, description as prod_id from walmart group by 1,2")
View(prod_id)
plot(subrules2, method="paracoord", control=list(reorder=TRUE))
subrules2 <- head(sort(rules, by="lift"), 10)
plot(subrules2, method="paracoord", control=list(reorder=TRUE))
subrules2 <- head(sort(rules, by="lift"), 20)
plot(subrules2, method="graph")
inspect(rules[1:5])
View(bad_transactions)
registers <-group_by(bad_transactions, register) %>% summarize(amount = n())
bad_registers <-group_by(bad_transactions, register) %>% summarize(amount = n())
View(bad_registers)
View(bad_transactions)
bad_transactions$amount <- -(bad_transactions$amount)
View(bad_transactions)
bad_registers <-group_by(bad_transactions, register) %>% summarize(amount = n())
View(bad_registers)
bad_registers <-group_by(bad_transactions, register) %>% summarize(amount = sum())
View(bad_registers)
bad_registers <-group_by(bad_transactions, register) %>% summarize(amount = n())
View(bad_registers)
sum(bad_transactions$amount)
sum(bad_registers$amount)
registers <-group_by(walmart, register) %>% summarize(amount = sum(amount))
View(registers)
summary(lm(register~amount, data =registers))
bad_registers <-group_by(bad_transactions, register) %>% summarize(amount = sum(amount))
View(bad_registers)
bad_registers <-group_by(bad_transactions, register) %>% summarize(deduction = sum(amount))
register_perf <- merge(registers, bad_registers by = "register")
register_perf <- merge(registers, bad_registers, by = "register")
View(register_perf)
register_perf$error_rate <- register_perf$deduction/register_perf$amount
View(register_perf)
inspect(register_perf$error_rate[1:5])
inspect(register_perf[1:5])
inspect(register_perf[1:5,])
header(register_perf[1:5,])
head(register_perf[1:5,])
register_perf[-order(register_perf$error_rate),]
head(register_perf[1:5,])
register_perf[order(register_perf$error_rate),]
head(register_perf[1:5,])
register_perf[order(register_perf$error_rate),]
register_perf[-order(register_perf$error_rate),]
register_perf[-order(register_perf$error_rate),]
head(register_perf[1:5,])
register_perf <- register_perf[-order(register_perf$error_rate),]
head(register_perf[1:5,])
register_perf <- merge(registers, bad_registers, by = "register")
register_perf$error_rate <- register_perf$deduction/register_perf$amount
register_perf <- register_perf[order(register_perf$error_rate, decreasing = T),]
head(register_perf[1:5,])
sam <- data[data$store_number == 3333,]
View(sam)
hour_bill_trend_sam <- sqldf("select Day, Hour, count(distinct order_id) as unique_bills,
sum(amount) as sales, count(upc) as products from sam group by 1,2")
hour_bill_trend_sam$bill_value <- hour_bill_trend$sales/hour_bill_trend$unique_bills
ggplot(hour_bill_trend_sam, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend_sam$Hour), max(hour_bill_trend_sam$Hour)))
ggplot(hour_bill_trend_sam, aes(Hour,unique_bills)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend_sam$Hour), max(hour_bill_trend_sam$Hour)))
ggplot(hour_bill_trend_sam, aes(Hour,products)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend_sam$Hour), max(hour_bill_trend_sam$Hour)))
ggplot(hour_bill_trend_sam, aes(Hour,sales)) + geom_line(aes(color=Day, group=Day), size=2)+geom_smooth()+
scale_x_continuous(breaks = seq(min(hour_bill_trend_sam$Hour), max(hour_bill_trend_sam$Hour)))
department_trend_sat_peak <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from peak group by 1")
department_trend_sam <- sqldf("select department,sum(amount) as sales,
count(distinct order_id) as unique_bills from sam group by 1")
View(department_trend_sam)
sam_products <- sqldf("select department, description, count(description) as count,
sum(amount) as sales from peak group by 1,2")
View(sam_products)
dept_56 <- sam[sam$department == 56,]
View(dept_56)
View(dept_56)
View(sam_products)
View(department_trend_sam)
dept_41 <- sam[sam$department == 41,]
View(dept_41)
registers <-group_by(data, register) %>% summarize(amount = sum(amount))
bad_transactions$amount <- -(bad_transactions$amount)
bad_registers <-group_by(bad_transactions, register) %>% summarize(deduction = sum(amount))
register_perf <- merge(registers, bad_registers, by = "register")
register_perf$error_rate <- register_perf$deduction/register_perf$amount
register_perf <- register_perf[order(register_perf$error_rate, decreasing = T),]
head(register_perf[1:5,])
View(registers)
bad_transactions$amount <- -(bad_transactions$amount)
View(bad_transactions)
bad_registers <-group_by(bad_transactions, register) %>% summarize(deduction = sum(amount))
register_perf <- merge(registers, bad_registers, by = "register")
register_perf$error_rate <- register_perf$deduction/register_perf$amount
register_perf <- register_perf[order(register_perf$error_rate, decreasing = T),]
head(register_perf[1:5,])
prod_id<-sqldf("select order_id as trans_id, description as prod_id from data group by 1,2")
write.csv(prod_id, "prod_id.csv",row.names=FALSE)
#Read it in as a transaction
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
#apply apriori package with a support minima of 0.001, and confidence limit of 95%
rules<-apriori(data=WM, parameter=list(supp=0.001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
#sort rules by highest confidence
rules<-sort(rules, decreasing=TRUE,by="confidence")
#inspect top 5 rules
inspect(rules[1:5])
#convert rules to dataframe for ease of view
final = as(rules, "data.frame")
#create a set of subrules to be visualized and plot using the igraph library
subrules2 <- head(sort(rules, by="lift"), 20)
plot(subrules2, method="graph")
prod_id<-sqldf("select order_id as trans_id, description as prod_id from data group by 1,2")
write.csv(prod_id, "prod_id.csv",row.names=FALSE)
#Read it in as a transaction
WM = read.transactions("prod_id.csv", format = "single", sep = ",", cols = c(1,2))
#apply apriori package with a support minima of 0.001, and confidence limit of 95%
rules<-apriori(data=WM, parameter=list(supp=0.001,conf = 0.05,minlen=2,maxlen=2),control = list(verbose=F))
#sort rules by highest confidence
rules<-sort(rules, decreasing=TRUE,by="confidence")
#inspect top 5 rules
inspect(rules[1:5])
#convert rules to dataframe for ease of view
final = as(rules, "data.frame")
#create a set of subrules to be visualized and plot using the igraph library
subrules2 <- head(sort(rules, by="lift"), 20)
summary(data)
